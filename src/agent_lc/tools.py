# src/agent_lc/tools.py
import json
from io import StringIO
import pandas as pd
import numpy as np
from langchain.tools import tool
from typing import Dict, Any, List

from .memory import combine_hist_for_row, mem_upsert_after_decision
from .risk import compute_prior, apply_hard_rules, label_to_prob, mix_final, llm_hint_floor
from .llm import call_llm


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –î–æ–º–µ–Ω–Ω—ã–µ —Ö–µ–ª–ø–µ—Ä—ã (—Ñ–ª–∞–≥–∏/–æ–±—ä—è—Å–Ω–µ–Ω–∏—è)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
HIGH_WORDS = {
    "–∑–∞–π–º","–¥–æ–≥–æ–≤–æ—Ä –∑–∞–π–º–∞","–≤–æ–∑–≤—Ä–∞—Ç –∑–∞–π–º–∞","–≤–∑–∞–∏–º–æ—Ä–∞—Å—á—ë—Ç","–ø–µ—Ä–µ–≤–æ–¥ —Å—Ä–µ–¥—Å—Ç–≤","–±–µ–∑ –¥–æ–≥–æ–≤–æ—Ä–∞",
    "–ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∫–∞—Ä—Ç—É","–ª–∏—á–Ω—ã–µ –Ω—É–∂–¥—ã","–∫—Ä–∏–ø—Ç–æ","–±–∏—Ç–∫–æ–∏–Ω","usdt","–±–∏—Ä–∂–∞","coin","crypto",
    "swift","–∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥","–≤–∞–ª—é—Ç–Ω—ã–π —Å—á—ë—Ç","—ç–∫—Å–ø–æ—Ä—Ç","–ø–µ—Ä–µ–¥–∞—á–∞ –∞–∫—Ç–∏–≤–æ–≤",
    "–ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ","–Ω–∞–ª–∏—á–Ω—ã–µ","–≤—ã–¥–∞—á–∞ –Ω–∞–ª–∏—á–Ω—ã—Ö","–æ–±–Ω–∞–ª–∏—á–∏–≤–∞–Ω–∏–µ",
    "–±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å","–ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏–µ","–∞–≥–µ–Ω—Ç—Å–∫–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ","–∫–æ–º–∏—Å—Å–∏–æ–Ω–Ω–æ–µ"
}

def _is_round(amount):
    try:
        a = float(amount)
        return (a % 10000 == 0) or (a % 100000 == 0)
    except Exception:
        return False

def _flags_from_row(r: dict) -> list[str]:
    flags = []
    cl, cd = r.get("chain_length"), r.get("chain_duration_hours")
    if cd is not None and float(cd) < 0.01:
        flags.append("transit_very_short")
    if (cl is not None and cd is not None and int(cl) >= 3 and float(cd) < 24) or (cd is not None and float(cd) < 24):
        flags.append("transit_short")
    if float(r.get("anomaly_amount", 0) or 0) >= 0.6:
        flags.append("amount_anomaly_strong")
    if float(r.get("anomaly_frequency", 0) or 0) >= 0.6:
        flags.append("freq_anomaly_strong")
    if float(r.get("anomaly_purpose", 0) or 0) >= 0.6:
        flags.append("purpose_anomaly")
    if _is_round(r.get("amount")):
        flags.append("round_large_amount")
    p = str(r.get("purpose","")).lower()
    if any(w in p for w in HIGH_WORDS):
        flags.append("purpose_stopword_high")
    # memory-* –ø–æ–¥—Å–∫–∞–∑–∫–∏ (–∏–∑ –ø–∞–º—è—Ç–∏ –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–æ–≤)
    if r.get("debit_watchlisted") or r.get("credit_watchlisted"):
        flags.append("memory_watchlist")
    if float(r.get("debit_susp_rate",0))>=0.4 or float(r.get("credit_susp_rate",0))>=0.4:
        flags.append("memory_high_susp_rate")
    if float(r.get("debit_cnt_suspicious",0))>=10 or float(r.get("credit_cnt_suspicious",0))>=10:
        flags.append("memory_many_past_flags")
    if float(r.get("debit_last_seen_days",1e6))<30 or float(r.get("credit_last_seen_days",1e6))<30:
        flags.append("memory_recent_activity")
    if (r.get("debit_p95") and r.get("amount") and float(r["amount"])>float(r["debit_p95"])) or \
       (r.get("credit_p95") and r.get("amount") and float(r["amount"])>float(r["credit_p95"])):
        flags.append("memory_above_p95")
    return flags

def _reasons_from_row(r: dict) -> list[str]:
    out=[]
    mm = r.get("ml_metric", None)
    if mm is not None:
        try: out.append(f"ml_metric={round(float(mm or 0), 2)}")
        except: pass
    aa = float(r.get("anomaly_amount",0) or 0)
    if aa>=0.6: out.append(f"–∞–Ω–æ–º–∞–ª–∏—è —Å—É–º–º—ã={aa:.2f}")
    af = float(r.get("anomaly_frequency",0) or 0)
    if af>=0.6: out.append(f"–∞–Ω–æ–º–∞–ª–∏—è —á–∞—Å—Ç–æ—Ç—ã={af:.2f}")
    ap = float(r.get("anomaly_purpose",0) or 0)
    if ap>=0.6: out.append(f"–∞–Ω–æ–º–∞–ª–∏—è –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è={ap:.2f}")
    if _is_round(r.get("amount")): out.append("–∫—Ä—É–≥–ª–∞—è —Å—É–º–º–∞")
    cl, cd = r.get("chain_length"), r.get("chain_duration_hours")
    try:
        if cl is not None and cd is not None and int(cl)>=3 and float(cd)<24:
            out.append("–∫–æ—Ä–æ—Ç–∫–∞—è —Ç—Ä–∞–Ω–∑–∏—Ç–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞")
    except: pass
    return out[:5]

def _merge_flags_and_reasons(t: dict, base: dict) -> dict:
    gen_flags = _flags_from_row(base)
    gen_reas  = _reasons_from_row(base)
    t.setdefault("flags", [])
    t.setdefault("primary_reasons", [])
    if not t["flags"]:
        t["flags"] = gen_flags
    else:
        seen = set(t["flags"])
        t["flags"].extend([f for f in gen_flags if f not in seen])
    if not t["primary_reasons"]:
        t["primary_reasons"] = gen_reas
    else:
        seen = set(t["primary_reasons"])
        t["primary_reasons"].extend([r for r in gen_reas if r not in seen])
    return t

_BAD_OK_PHRASES = (
    "–Ω–µ –≤—ã–∑—ã–≤–∞–µ—Ç –ø–æ–¥–æ–∑—Ä–µ–Ω–∏–π", "–Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ–π", "—Ç–∏–ø–∏—á–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è",
    "–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∏—Å–∫–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ", "—Ä–∏—Å–∫ –Ω–∏–∑–∫–∏–π", "–±–µ–∑–æ–ø–∞—Å–Ω–∞", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è"
)

def _enforce_text_consistency(tr: dict) -> dict:
    """–ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–µ—Ç–∫–µ, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ —à–∞–±–ª–æ–Ω—ã –ø–æ –º–µ—Ç–∫–µ."""
    lbl = (tr.get("risk_label") or "").lower()
    rec = (tr.get("recommendation") or "").lower()
    expl = (tr.get("risk_explanation") or "").lower()

    def has_ok_phrases(s: str) -> bool:
        return any(p in s for p in _BAD_OK_PHRASES)

    if lbl == "–∫—Ä–∞—Å–Ω—ã–π":
        # —Ç–µ–∫—Å—Ç –æ–±—è–∑–∞–Ω –±—ã—Ç—å ¬´–∂—ë—Å—Ç–∫–∏–º¬ª, —É–±–∏—Ä–∞–µ–º –ª—é–±—ã–µ ¬´–≤—Å—ë –æ–∫¬ª
        if has_ok_phrases(rec) or not tr.get("recommendation"):
            tr["recommendation"] = (
                "–°–≤—è–∂–∏—Ç–µ—Å—å —Å –∫–ª–∏–µ–Ω—Ç–æ–º –∏ –∑–∞–ø—Ä–æ—Å–∏—Ç–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã. "
                "–ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏."
            )
        if has_ok_phrases(expl) or not tr.get("risk_explanation"):
            tr["risk_explanation"] = (
                "–í—ã—è–≤–ª–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–≤—ã—à–µ–Ω–Ω–æ–≥–æ —Ä–∏—Å–∫–∞ –ø–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—é/–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞–º/—Å—É–º–º–µ. "
                "–û–ø–µ—Ä–∞—Ü–∏—è –≤—ã–≥–ª—è–¥–∏—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∏ —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏."
            )

    elif lbl == "–∂–µ–ª—Ç—ã–π":
        # —É–º–µ—Ä–µ–Ω–Ω–æ –Ω–∞—Å—Ç–æ—Ä–æ–∂–µ–Ω–Ω—ã–π —Ç–æ–Ω
        if has_ok_phrases(rec) or not tr.get("recommendation"):
            tr["recommendation"] = (
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –ø–ª–∞—Ç–µ–∂–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã. "
                "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ—Ü–µ–Ω–∏—Ç–µ –∏—Å—Ç–æ—Ä–∏—é –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞."
            )
        if has_ok_phrases(expl) or not tr.get("risk_explanation"):
            tr["risk_explanation"] = (
                "–ï—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç–æ—Ä–∞–∂–∏–≤–∞—é—â–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã, –Ω–æ –±–µ–∑ —è–≤–Ω—ã—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π. "
                "–¢—Ä–µ–±—É–µ—Ç—Å—è —É—Ç–æ—á–Ω—è—é—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞."
            )

    else:  # –∑–µ–ª—ë–Ω—ã–π
        # —è–≤–Ω–æ —Å–æ–æ–±—â–∞–µ–º, —á—Ç–æ –ø–æ–¥–æ–∑—Ä–µ–Ω–∏–π –Ω–µ—Ç
        if not tr.get("recommendation"):
            tr["recommendation"] = "–•—Ä–∞–Ω–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –æ–ø–µ—Ä–∞—Ü–∏–∏. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π."
        if not tr.get("risk_explanation") or not has_ok_phrases(expl):
            tr["risk_explanation"] = (
                "–û–ø–µ—Ä–∞—Ü–∏—è –≤—ã–≥–ª—è–¥–∏—Ç —Ç–∏–ø–∏—á–Ω–æ–π –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ –∏ –Ω–µ –≤—ã–∑—ã–≤–∞–µ—Ç –ø–æ–¥–æ–∑—Ä–µ–Ω–∏–π. "
                "–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∏ —Å—É–º–º–∞ –≤ –æ–±—ã—á–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö."
            )

    return tr

def _fill_missing(tr: dict) -> dict:
    def _short_expl(t):
        fl  = t.get("flags", [])[:3]
        prs = t.get("primary_reasons", [])[:2]
        parts=[]
        if fl:  parts.append(", ".join(fl))
        if prs: parts.append("; ".join(prs))
        return (" ".join(parts)).strip() or "–û–ø–µ—Ä–∞—Ü–∏—è –≤—ã–≥–ª—è–¥–∏—Ç —Ç–∏–ø–∏—á–Ω–æ–π –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞. –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∏—Å–∫–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ."
    if not tr.get("risk_explanation"):
        tr["risk_explanation"] = _short_expl(tr)
    if not tr.get("recommendation"):
        lbl = tr.get("risk_label","–∑–µ–ª–µ–Ω—ã–π")
        rec = {
            "–∫—Ä–∞—Å–Ω—ã–π": "–°–≤—è–∂–∏—Ç–µ—Å—å —Å –∫–ª–∏–µ–Ω—Ç–æ–º. –ó–∞–ø—Ä–æ—Å–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –æ–ø–µ—Ä–∞—Ü–∏—é.",
            "–∂–µ–ª—Ç—ã–π":  "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –ø–ª–∞—Ç–µ–∂–∞. –ü—Ä–æ–≤–µ–¥–∏—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞.",
            "–∑–µ–ª–µ–Ω—ã–π": "–•—Ä–∞–Ω–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –æ–ø–µ—Ä–∞—Ü–∏–∏. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π."
        }.get(lbl, "–ü—Ä–æ–≤–µ–¥–∏—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –ø–ª–∞—Ç–µ–∂–∞.")
        tr["recommendation"] = rec
    ev = tr.setdefault("evidence", {})
    for k in ["ml_metric","anomaly_amount","amount","chain_match","chain_length","chain_duration_hours"]:
        ev.setdefault(k, None)
    return tr


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–µ–ª–ø–µ—Ä—ã (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤/–æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _to_float(x):
    try:
        if pd.isna(x):  # NaN/NA -> None
            return None
        return float(x)
    except Exception:
        return None

def _to_int_or_none(x):
    try:
        if pd.isna(x):
            return None
        return int(x)
    except Exception:
        return None

def _to_iso(x):
    """Timestamp/datetime -> ISO string; –∏–Ω–∞—á–µ None/str –∫–∞–∫ –µ—Å—Ç—å."""
    if x is None or (isinstance(x, float) and np.isnan(x)):
        return None
    try:
        if isinstance(x, pd.Timestamp):
            return x.isoformat()
        if isinstance(x, str):
            return x
        ts = pd.to_datetime(x, errors="coerce")
        return ts.isoformat() if pd.notna(ts) else (str(x) if x is not None else None)
    except Exception:
        return str(x) if x is not None else None

def _to_jsonable(v):
    """–ü—Ä–∏–≤–æ–¥–∏—Ç numpy/scalars/TS –∫ json-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º —Ç–∏–ø–∞–º."""
    if isinstance(v, (np.integer,)):
        return int(v)
    if isinstance(v, (np.floating,)):
        return None if np.isnan(v) else float(v)
    if isinstance(v, (pd.Timestamp, np.datetime64)):
        return _to_iso(v)
    return v

def _round2(x):
    try:
        return None if x is None else round(float(x), 2)
    except Exception:
        return None


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# TOOL: —Å–±–æ—Ä payload –¥–ª—è LLM (—Å –ø–∞–º—è—Ç—å—é)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@tool("build_llm_payload", return_direct=True)
def build_llm_payload_tool(df_json: str) -> str:
    """–í—Ö–æ–¥: JSON df (records). –í—ã—Ö–æ–¥: –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è LLM (—Å –ø–∞–º—è—Ç—å—é)."""
    df = pd.read_json(StringIO(df_json), orient="records")
    df = df.reset_index(drop=True)

    rows: List[Dict[str, Any]] = []
    for i, r in df.iterrows():
        rid = _to_int_or_none(r.get("id")) or (i + 1)

        # –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π, —Ç–æ–∫–µ–Ω–æ-—ç–∫–æ–Ω–æ–º–Ω—ã–π Payload
        row = {
            "id": rid,
            "purpose": str(r.get("purpose", ""))[:300],   # –æ–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª–∏–Ω—É
            "ml_metric": _round2(r.get("ml_metric")),
            "anomaly_amount": _round2(r.get("anomaly_amount")),
            "anomaly_frequency": _round2(r.get("anomaly_frequency")),
            "anomaly_purpose": _round2(r.get("anomaly_purpose")),
            "anomaly_overall": _round2(r.get("anomaly_overall")),
            "is_regular_payment": bool(r.get("is_regular_payment", False)),
            "debit_name_type": str(r.get("debit_name_type", "–ü—Ä–æ—á–µ–µ")),
            "credit_name_type": str(r.get("credit_name_type", "–ü—Ä–æ—á–µ–µ")),
            "debit_amount": _round2(r.get("debit_amount")),
            "credit_amount": _round2(r.get("credit_amount")),
            "amount": _round2(r.get("amount")),
            "debit_inn": "" if pd.isna(r.get("debit_inn")) else str(r.get("debit_inn")),
            "credit_inn": "" if pd.isna(r.get("credit_inn")) else str(r.get("credit_inn")),
            "chain_match": None if pd.isna(r.get("chain_id")) else str(r.get("chain_id")),
            "chain_length": _to_int_or_none(r.get("chain_length")),
            "chain_duration_hours": _round2(r.get("chain_duration_hours")),
            # TS ‚Üí ISO (–∫–æ—Ä–æ—Ç–∫–∏–π –≤–∏–¥ –¥–æ —Å–µ–∫—É–Ω–¥)
            "ts": (_to_iso(r.get("ts") or r.get("date")) or "")[:19],
        }

        # üî∂ –ü–ê–ú–Ø–¢–¨: –ø–æ–¥–º–µ—à–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–æ–≤ (SQLite)
        hist = combine_hist_for_row(row)
        # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è, –æ–∫—Ä—É–≥–ª—è–µ–º
        keep = {
            "debit_susp_rate","debit_cnt_suspicious","debit_last_seen_days","debit_watchlisted","debit_p95",
            "credit_susp_rate","credit_cnt_suspicious","credit_last_seen_days","credit_watchlisted","credit_p95"
        }
        hist = {k: _round2(v) if isinstance(v,(int,float)) else v for k,v in hist.items() if k in keep}

        # –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–∏—á–∏–Ω—ã –æ—Ç ML ‚Äî —à–ª—ë–º, –Ω–æ –Ω–µ –∑–∞—Å–æ—Ä—è–µ–º –ø—É—Å—Ç—ã–º
        ml_reasons = r.get("ml_top_reasons", [])
        if isinstance(ml_reasons, list) and ml_reasons:
            row["ml_top_reasons"] = ml_reasons[:5]

        # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π jsonable row
        row = {**{k: _to_jsonable(v) for k, v in row.items()}, **{k: _to_jsonable(v) for k, v in hist.items()}}
        rows.append(row)

    return json.dumps({"transactions": rows, "input_len": len(rows)}, ensure_ascii=False)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# TOOL: –≤—ã–∑–æ–≤ LLM + —Å–º–µ—à–∏–≤–∞–Ω–∏–µ —Å ML/Prior/Rules + –ª–æ–≥ –≤ –ø–∞–º—è—Ç—å
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@tool("llm_assess_risk", return_direct=True)
def llm_assess_risk_tool(enriched_rows_json: str) -> str:
    """–í—Ö–æ–¥: JSON enriched rows. –í—ã—Ö–æ–¥: —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (ML+prior+LLM+rules) + –ª–æ–≥ –≤ –ø–∞–º—è—Ç—å."""
    payload = json.loads(enriched_rows_json) if isinstance(enriched_rows_json, str) else enriched_rows_json
    rows_enriched: List[Dict[str, Any]] = payload.get("transactions", [])

    # 1) –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ LLM + –æ–±—ä—è—Å–Ω–µ–Ω–∏—è (—É—Å—Ç–æ–π—á–∏–≤–æ)
    try:
        data = call_llm(rows_enriched)
        tx = data.get("transactions", [])
    except Exception:
        # üîÅ Fallback: –µ—Å–ª–∏ LLM –æ–±–æ—Ä–≤–∞–ª—Å—è/–æ—à–∏–±–∫–∞ ‚Äî —Å—á–∏—Ç–∞–µ–º –±–µ–∑ LLM, —á—Ç–æ–±—ã –æ—Ç—á—ë—Ç –Ω–µ –±—ã–ª –ø—É—Å—Ç—ã–º
        tx = []
        for base in rows_enriched:
            p_ml = float(base.get("ml_metric", 0.0) or 0.0)
            hist = {
                "debit_cnt_suspicious": base.get("debit_cnt_suspicious", 0.0),
                "debit_susp_rate": base.get("debit_susp_rate", 0.0),
                "debit_last_seen_days": base.get("debit_last_seen_days", 1e6),
                "debit_watchlisted": base.get("debit_watchlisted", 0),
                "debit_p95": base.get("debit_p95"),
                "credit_cnt_suspicious": base.get("credit_cnt_suspicious", 0.0),
                "credit_susp_rate": base.get("credit_susp_rate", 0.0),
                "credit_last_seen_days": base.get("credit_last_seen_days", 1e6),
                "credit_watchlisted": base.get("credit_watchlisted", 0),
                "credit_p95": base.get("credit_p95"),
            }
            p_prior, _ = compute_prior(hist, base)
            p_llm = 0.2  # –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ –∑–µ–ª—ë–Ω—ã–π
            hard_hit, rule_ids = apply_hard_rules(base, hist)
            p_final, is_suspicious, label = mix_final(p_ml, p_prior, p_llm, hard_hit)

            t = {
                "id": int(base.get("id")),
                "purpose": base.get("purpose",""),
                "risk_label": label,
                "risk_score": float(round(p_final, 2)),
                "flags": [],
                "primary_reasons": [],
                "evidence": {}
            }
            # —Ñ–ª–∞–≥–∏/–ø—Ä–∏—á–∏–Ω—ã + evidence + —Ç–µ–∫—Å—Ç—ã
            t = _merge_flags_and_reasons(t, base)
            ev = t.get("evidence", {})
            ev.update({"ml_metric": p_ml, "prior": round(p_prior,3), "p_llm": round(p_llm,3), "p_final": round(p_final,3)})
            t["evidence"]=ev; t["rule_hits"]=rule_ids
            t = _fill_missing(t)
            # –ø–∏—à–µ–º –≤ –ø–∞–º—è—Ç—å
            mem_upsert_after_decision(
                dict(id=int(base.get("id")), ts=base.get("ts"),
                     debit_inn=base.get("debit_inn"), credit_inn=base.get("credit_inn"),
                     amount=base.get("amount"), purpose=base.get("purpose")),
                dict(p_ml=p_ml, p_prior=p_prior, p_llm=p_llm, p_final=p_final,
                     label_pred=label, is_suspicious=is_suspicious,
                     rule_hits=rule_ids, reasons_llm=t.get("primary_reasons", []))
            )
            tx.append({k: _to_jsonable(v) for k, v in t.items()})

        return json.dumps({"overall_observation": "", "transactions": tx}, ensure_ascii=False)

    # 2) –û—Å–Ω–æ–≤–Ω–æ–π –ø—É—Ç—å: –µ—Å—Ç—å –æ—Ç–≤–µ—Ç LLM ‚Üí —Å–º–µ—à–∏–≤–∞–µ–º –∏ –ª–æ–≥–∏—Ä—É–µ–º
    final_tx: List[Dict[str, Any]] = []
    for t in tx:
        rid = _to_int_or_none(t.get("id"))
        base = next((r for r in rows_enriched if _to_int_or_none(r.get("id")) == rid), {})

        p_ml = float(base.get("ml_metric", 0.0) or 0.0)
        hist = {
            "debit_cnt_suspicious": base.get("debit_cnt_suspicious", 0.0),
            "debit_susp_rate": base.get("debit_susp_rate", 0.0),
            "debit_last_seen_days": base.get("debit_last_seen_days", 1e6),
            "debit_watchlisted": base.get("debit_watchlisted", 0),
            "debit_p95": base.get("debit_p95"),
            "credit_cnt_suspicious": base.get("credit_cnt_suspicious", 0.0),
            "credit_susp_rate": base.get("credit_susp_rate", 0.0),
            "credit_last_seen_days": base.get("credit_last_seen_days", 1e6),
            "credit_watchlisted": base.get("credit_watchlisted", 0),
            "credit_p95": base.get("credit_p95"),
        }

        # prior / llm / –ø—Ä–∞–≤–∏–ª–∞
        p_prior, _ = compute_prior(hist, base)
        p_llm = label_to_prob(t.get("risk_label"), t.get("risk_score"))
        hard_hit, rule_ids = apply_hard_rules(base, hist)

        # üî∏ LLM-floor: –µ—Å–ª–∏ LLM ¬´–∫—Ä–∞—Å–Ω—ã–π¬ª, –Ω–µ –æ–ø—É—Å–∫–∞–µ–º –∏—Ç–æ–≥ –Ω–∏–∂–µ –º—è–≥–∫–æ–≥–æ –ø–æ—Ä–æ–≥–∞
        floor_hint = llm_hint_floor(base, p_llm)

        # —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Å–º–µ—Å—å
        p_final, is_suspicious, label = mix_final(p_ml, p_prior, p_llm, hard_hit, llm_floor=floor_hint)

        # –∏—Ç–æ–≥–æ–≤—ã–µ –ø–æ–ª—è ‚Üí —á—Ç–æ–±—ã _fill_missing –≤–∏–¥–µ–ª —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–µ—Ç–∫—É
        t["risk_label"] = label
        t["risk_score"] = float(round(p_final, 2))
        t["rule_hits"] = rule_ids

        # –∞–≤—Ç–æ–¥–æ–±–∞–≤–∏–º —Ñ–ª–∞–≥–∏/–ø—Ä–∏—á–∏–Ω—ã
        t = _merge_flags_and_reasons(t, base)

        # evidence + –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        ev = t.get("evidence", {}) or {}
        ev.update({
            "ml_metric": p_ml,
            "prior": round(p_prior, 3),
            "p_llm": round(p_llm, 3),
            "p_final": round(p_final, 3),
        })
        t["evidence"] = ev

        # –∑–∞–ø–æ–ª–Ω–∏—Ç—å –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã (—É—á–∏—Ç—ã–≤–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–µ—Ç–∫—É)
        t = _fill_missing(t)

        t = _enforce_text_consistency(t)

        # üî∂ –õ–û–ì –í –ü–ê–ú–Ø–¢–¨ (–∏ –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ç—ã –±—É–¥—É—Ç –ø–µ—Ä–µ—Å—á–∏—Ç–∞–Ω—ã –≤ memory.py)
        mem_upsert_after_decision(
            dict(id=rid,
                 ts=base.get("ts"),
                 debit_inn=base.get("debit_inn"),
                 credit_inn=base.get("credit_inn"),
                 amount=base.get("amount"),
                 purpose=base.get("purpose")),
            dict(p_ml=p_ml, p_prior=p_prior, p_llm=p_llm, p_final=p_final,
                 label_pred=label, is_suspicious=is_suspicious,
                 rule_hits=rule_ids, reasons_llm=t.get("primary_reasons", []))
        )

        # json-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –Ω–∞ –≤—ã—Ö–æ–¥–µ
        final_tx.append({k: _to_jsonable(v) for k, v in t.items()})

    return json.dumps(
        {"overall_observation": data.get("overall_observation", ""), "transactions": final_tx},
        ensure_ascii=False
    )
