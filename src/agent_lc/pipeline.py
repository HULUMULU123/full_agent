# src/agent_lc/pipeline.py
import json
import time
import pandas as pd
from langchain_core.runnables import RunnableSequence

from .memory import mem_init, mem_bulk_preload_statement
from .features import build_base_features
from .model import load_artifacts, predict_with_pipeline
from .tools import build_llm_payload_tool, llm_assess_risk_tool
from .export import export_excel_report

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# try/except –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
try:
    from tqdm import tqdm  # optional
    _HAS_TQDM = True
except Exception:
    _HAS_TQDM = False


def _read_csv_robust(path: str) -> pd.DataFrame:
    tried = []
    for enc in ("utf-8", "utf-8-sig", "cp1251", "latin1"):
        for sep in (",", ";", "\t", "|"):
            try:
                return pd.read_csv(path, encoding=enc, sep=sep)
            except Exception as e:
                tried.append(f"{enc}/{repr(sep)} -> {e.__class__.__name__}")
                continue
    return pd.read_csv(path, engine="python", encoding_errors="replace", sep=None)


def _ensure_ids(df: pd.DataFrame) -> pd.DataFrame:
    df = df.reset_index(drop=True)
    if "id" not in df.columns:
        df["id"] = df.index + 1
    else:
        s = pd.to_numeric(df["id"], errors="coerce")
        # –∑–∞–ø–æ–ª–Ω–∏–º NaN –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é 1..N
        fill_seq = pd.Series(range(1, len(df) + 1), index=df.index)
        s = s.fillna(fill_seq)
        s[s <= 0] = fill_seq[s <= 0]
        df["id"] = s.astype(int)
    return df


def run_pipeline(csv_path: str, out_xlsx: str, llm_batch_size: int = 10, verbose: bool = True) -> dict:
    # 1) –ü–∞–º—è—Ç—å/–ë–î
    mem_init()

    # 2) –î–∞–Ω–Ω—ã–µ
    df_raw = _read_csv_robust(csv_path)

    # 3) –ü—Ä–∏–∑–Ω–∞–∫–∏
    df_prep = build_base_features(df_raw)

    # 4) –ú–æ–¥–µ–ª—å
    pipe, _ = load_artifacts()
    df_scored = predict_with_pipeline(pipe, df_prep)
    df_scored = _ensure_ids(df_scored)

    # 4.5) üî∂ –ü–†–ï–î–ó–ê–ì–†–£–ó–ö–ê –í–°–ï–ô –í–´–ü–ò–°–ö–ò –í –ü–ê–ú–Ø–¢–¨ (tx + agg_counterparty)
    # –≠—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã PRIOR/–∫–≤–∞–Ω—Ç–∏–ª–∏/last_seen —É–∂–µ —É—á–∏—Ç—ã–≤–∞–ª–∏ –≤—Å—é —Ç–∞–±–ª–∏—Ü—É –¥–æ LLM.
    mem_bulk_preload_statement(df_scored)

    # 5) –û—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è LLM –ü–û –ë–ê–¢–ß–ê–ú (–∫–∞–∫ –±—ã–ª–æ)
    chain = RunnableSequence(first=build_llm_payload_tool, last=llm_assess_risk_tool)

    all_records = json.loads(df_scored.to_json(orient="records", force_ascii=False))
    merged_tx = []

    if not all_records:
        raise RuntimeError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.")

    total = len(all_records)
    total_batches = (total + llm_batch_size - 1) // llm_batch_size

    # ‚îÄ‚îÄ –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    start_ts = time.time()
    if verbose:
        if _HAS_TQDM:
            pbar = tqdm(total=total, desc=f"LLM batches (size={llm_batch_size})", unit="tx")
        else:
            print(f"[LLM] –ó–∞–ø—É—Å–∫ –ø–æ –ø–∞–∫–µ—Ç–∞–º: –≤—Å–µ–≥–æ {total} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π, "
                  f"batch_size={llm_batch_size}, batches={total_batches}")

    processed = 0
    for bi in range(0, total, llm_batch_size):
        batch_records = all_records[bi:bi + llm_batch_size]
        batch_idx = bi // llm_batch_size + 1

        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä (plain)
        if verbose and not _HAS_TQDM:
            elapsed = time.time() - start_ts
            rate = processed / elapsed if elapsed > 0 else 0.0
            print(f"  - –ø–∞–∫–µ—Ç {batch_idx}/{total_batches} "
                  f"(rows {bi+1}..{bi+len(batch_records)}), "
                  f"–≥–æ—Ç–æ–≤–æ {processed}/{total} | {rate:.1f} tx/s")

        # –ü–µ—Ä–µ–¥–∞—ë–º –≤ –ø–µ—Ä–≤—ã–π tool –∏–º–µ–Ω–Ω–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ
        enriched_json = chain.invoke(json.dumps(batch_records, ensure_ascii=False))
        # –ù–∞–¥—ë–∂–Ω—ã–π –ø–∞—Ä—Å (–æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –ø—Ä–∏–Ω–∏–º–∞–µ–º)
        part = json.loads(enriched_json) if isinstance(enriched_json, str) else enriched_json
        got = len(part.get("transactions", []))
        merged_tx.extend(part.get("transactions", []))

        # –æ–±–Ω–æ–≤–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        processed += len(batch_records)
        if verbose and _HAS_TQDM:
            pbar.update(len(batch_records))
            pbar.set_postfix_str(f"batch={batch_idx}/{total_batches}, got={got}")

    if verbose and _HAS_TQDM:
        pbar.close()
    if verbose and not _HAS_TQDM:
        elapsed = time.time() - start_ts
        rate = processed / elapsed if elapsed > 0 else 0.0
        print(f"[LLM] –ì–æ—Ç–æ–≤–æ: {processed}/{total} –∑–∞ {elapsed:.1f}s ({rate:.1f} tx/s)")

    llm_resp = {"overall_observation": "", "transactions": merged_tx}

    # 6) Excel
    xlsx = export_excel_report(df_scored, llm_resp, out_xlsx)

    # 7) –°–≤–æ–¥–∫–∞
    lbls = [t.get("risk_label") for t in llm_resp.get("transactions", [])]
    summary = {
        "red":    sum(1 for x in lbls if x == "–∫—Ä–∞—Å–Ω—ã–π"),
        "yellow": sum(1 for x in lbls if x in ("–∂–µ–ª—Ç—ã–π", "–∂—ë–ª—Ç—ã–π")),
        "green":  sum(1 for x in lbls if x in ("–∑–µ–ª–µ–Ω—ã–π", "–∑–µ–ª—ë–Ω—ã–π")),
        "total":  len(lbls),
    }
    return {"xlsx": xlsx, "summary": summary}
